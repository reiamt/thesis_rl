
[2023-05-22 08:42:59,438, ema.py:67] EMA smoothing ENABLED: 0.99
[2023-05-22 08:43:02,715, data_pipeline.py:49] Imported 101_20200102_merge.csv.xz from a csv in 3 seconds
[2023-05-22 08:43:03,089, ema.py:93] Applying EMA to data...
[2023-05-22 08:43:05,294, data_pipeline.py:49] Imported /XBTUSD_2020-01-03.csv.xz from a csv in 1 seconds
[2023-05-22 08:43:05,523, ema.py:93] Applying EMA to data...
[2023-05-22 08:43:05,763, data_pipeline.py:228] Adding order imbalances...
[2023-05-22 08:43:05,802, ema.py:127] Reset EMA data.
[2023-05-22 08:43:05,802, ema.py:93] Applying EMA to data...
[2023-05-22 08:43:06,003, ema.py:67] EMA smoothing ENABLED: 0.99
[2023-05-22 08:43:06,003, ema.py:67] EMA smoothing ENABLED: 0.99
[2023-05-22 08:43:06,003, ema.py:67] EMA smoothing ENABLED: 0.99
[2023-05-22 08:43:06,004, ema.py:67] EMA smoothing ENABLED: 0.99
Resetting environment #1 on episode #0.
market-maker-v0 XBTUSD #1 instantiated
observation_space: (100, 174) reward_type = TRADE_COMPLETION max_steps = 86345
Using cpu device
Resetting environment #1 on episode #0.
Logging to ./runs/h3j17way/DQN_1
------------------------- XBTUSD-1 TRADE_COMPLETION EPISODE RESET -------------------------
Episode Reward: -408.9887
Episode PnL: -1.05%
Trade Count: 1702
Average PnL per Trade: -0.0061%
Total # of episodes: 1
short_inventory_market_orders	=	356
short_inventory_orders_placed	=	586
short_inventory_orders_updated	=	9802
short_inventory_orders_executed	=	585
long_inventory_market_orders	=	266
long_inventory_orders_placed	=	496
long_inventory_orders_updated	=	9884
long_inventory_orders_executed	=	495
First step:	5192
===========================================================================
----------------------------------------
| logger/                  |           |
|    episode_avg_trade_pnl | -6.14e-05 |
|    episode_pnl           | -1.05     |
|    episode_reward        | -409      |
| rollout/                 |           |
|    ep_len_mean           | 1.46e+04  |
|    ep_rew_mean           | -409      |
|    exploration_rate      | 0.722     |
| time/                    |           |
|    episodes              | 1         |
|    fps                   | 2049      |
|    time_elapsed          | 7         |
|    total_timesteps       | 14635     |
----------------------------------------
------------------------- XBTUSD-1 TRADE_COMPLETION EPISODE RESET -------------------------
Episode Reward: -383.9933
Episode PnL: -4.67%
Trade Count: 1772
Average PnL per Trade: -0.0264%
Total # of episodes: 2
short_inventory_market_orders	=	343
short_inventory_orders_placed	=	588
short_inventory_orders_updated	=	10737
short_inventory_orders_executed	=	587
long_inventory_market_orders	=	299
long_inventory_orders_placed	=	544
long_inventory_orders_updated	=	10819
long_inventory_orders_executed	=	543
First step:	10955
===========================================================================
----------------------------------------
| logger/                  |           |
|    episode_avg_trade_pnl | -0.000264 |
|    episode_pnl           | -4.67     |
|    episode_reward        | -384      |
| rollout/                 |           |
|    ep_len_mean           | 1.53e+04  |
|    ep_rew_mean           | -396      |
|    exploration_rate      | 0.417     |
| time/                    |           |
|    episodes              | 2         |
|    fps                   | 2125      |
|    time_elapsed          | 14        |
|    total_timesteps       | 30666     |
----------------------------------------
------------------------- XBTUSD-1 TRADE_COMPLETION EPISODE RESET -------------------------
Episode Reward: -364.9922
Episode PnL: -3.35%
Trade Count: 1686
Average PnL per Trade: -0.0199%
Total # of episodes: 3
short_inventory_market_orders	=	340
short_inventory_orders_placed	=	586
short_inventory_orders_updated	=	9892
short_inventory_orders_executed	=	585
long_inventory_market_orders	=	258
long_inventory_orders_placed	=	504
long_inventory_orders_updated	=	10030
long_inventory_orders_executed	=	503
First step:	7813
===========================================================================
----------------------------------------
| logger/                  |           |
|    episode_avg_trade_pnl | -0.000199 |
|    episode_pnl           | -3.35     |
|    episode_reward        | -365      |
| rollout/                 |           |
|    ep_len_mean           | 1.52e+04  |
|    ep_rew_mean           | -386      |
|    exploration_rate      | 0.135     |
| time/                    |           |
|    episodes              | 3         |
|    fps                   | 2125      |
|    time_elapsed          | 21        |
|    total_timesteps       | 45544     |
----------------------------------------
Traceback (most recent call last):
  File "agent/sb3.py", line 101, in <module>
    model.learn(
  File "/Users/tam/miniconda3/envs/thesis/lib/python3.8/site-packages/stable_baselines3/dqn/dqn.py", line 269, in learn
    return super().learn(
  File "/Users/tam/miniconda3/envs/thesis/lib/python3.8/site-packages/stable_baselines3/common/off_policy_algorithm.py", line 330, in learn
    self.train(batch_size=self.batch_size, gradient_steps=gradient_steps)
  File "/Users/tam/miniconda3/envs/thesis/lib/python3.8/site-packages/stable_baselines3/dqn/dqn.py", line 199, in train
    next_q_values = self.q_net_target(replay_data.next_observations)
  File "/Users/tam/miniconda3/envs/thesis/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/tam/miniconda3/envs/thesis/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py", line 66, in forward
    return self.q_net(self.extract_features(obs, self.features_extractor))
  File "/Users/tam/miniconda3/envs/thesis/lib/python3.8/site-packages/stable_baselines3/common/policies.py", line 130, in extract_features
    return features_extractor(preprocessed_obs)
  File "/Users/tam/miniconda3/envs/thesis/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/tam/miniconda3/envs/thesis/lib/python3.8/site-packages/stable_baselines3/common/torch_layers.py", line 45, in forward
    return self.flatten(observations)
  File "/Users/tam/miniconda3/envs/thesis/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1495, in _call_impl
    forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)
KeyboardInterrupt